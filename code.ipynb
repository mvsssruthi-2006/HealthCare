{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e45b77d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: qwen-vl-utils in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (0.0.10)\n",
      "Requirement already satisfied: filelock in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: av in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from qwen-vl-utils) (14.3.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from qwen-vl-utils) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mvsss\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d140426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing text data\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 0 documents\n",
      "Loaded 0 text documents and 0 image documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Qdrant: collections=[CollectionDescription(name='medical_rag_023a4f1e'), CollectionDescription(name='medical_rag_68945c5a'), CollectionDescription(name='medical_rag_7cb344da'), CollectionDescription(name='medical_rag_084f4311'), CollectionDescription(name='medical_rag_b000a0ed'), CollectionDescription(name='medical_rag_04df11e3'), CollectionDescription(name='medical_rag_c16f5bbd'), CollectionDescription(name='medical_rag_2cfc8102'), CollectionDescription(name='medical_rag_397ea9f0'), CollectionDescription(name='medical_rag_373c3969'), CollectionDescription(name='medical_rag_edffc0e0'), CollectionDescription(name='medical_rag_601bd9cd'), CollectionDescription(name='medical_rag_a64e6df8'), CollectionDescription(name='medical_f1ee0581'), CollectionDescription(name='medical_rag_84741282'), CollectionDescription(name='medical_rag_989e2ed8'), CollectionDescription(name='medical_rag_e4038d99'), CollectionDescription(name='medical_rag_b9e0087e'), CollectionDescription(name='medical_rag_b347d35f'), CollectionDescription(name='medical_rag_7df31907'), CollectionDescription(name='medical_rag_451f47a3'), CollectionDescription(name='medical_rag_47c2209c'), CollectionDescription(name='medical_rag_e8070cf2'), CollectionDescription(name='medical_rag_7c1702a5'), CollectionDescription(name='medical_rag_5de44867'), CollectionDescription(name='medical_rag_10ec921b'), CollectionDescription(name='medical_rag_0d8a7bce'), CollectionDescription(name='medical_rag_79b42a9b'), CollectionDescription(name='medical_rag_d86e3ff5'), CollectionDescription(name='medical_rag_d3351a50'), CollectionDescription(name='medical_rag_2ef0179a'), CollectionDescription(name='medical_rag_12405e2c'), CollectionDescription(name='medical_rag_ddc63914'), CollectionDescription(name='medical_rag_f408fab8'), CollectionDescription(name='medical_rag_5be527b1'), CollectionDescription(name='medical_rag_db1b83e4'), CollectionDescription(name='medical_rag_53fbc472'), CollectionDescription(name='medical_rag_f287b6c1'), CollectionDescription(name='medical_rag_9da7c2f8'), CollectionDescription(name='medical_rag_f054cb4e'), CollectionDescription(name='medical_e9227508'), CollectionDescription(name='medical_rag_9fb7abf8'), CollectionDescription(name='medical_rag_895bbb53'), CollectionDescription(name='medical_rag_e3915c11'), CollectionDescription(name='medical_rag_4c99304a'), CollectionDescription(name='medical_rag_7a8e4637'), CollectionDescription(name='medical_rag_18923dbf'), CollectionDescription(name='medical_rag_ca4aa241'), CollectionDescription(name='medical_rag_8629e835')]\n",
      "Created index with collection name: medical_rag_21979386\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "import uuid\n",
    "import shutil\n",
    "import pickle\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from PIL import Image\n",
    "import torch\n",
    "import base64\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "# Apply nest_asyncio for Jupyter compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Core RAG components\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, Document, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import qdrant_client\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# CLIP for image embeddings\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Embeddings\n",
    "from llama_index.embeddings.nomic import NomicEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# LLM setup\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "# UI components\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "# Set API keys\n",
    "llamaparse_api_key = \"Your llamaparse api key\"\n",
    "groq_api_key = \"Your Groq api key \"\n",
    "qdrant_url = \"Your qdrant url key\"\n",
    "qdrant_api_key = \"Your qdrant api key\"\n",
    "\n",
    "# Define data folders\n",
    "data_folder = \"Your data folder\"\n",
    "image_folder = os.path.join(data_folder, \"images\")\n",
    "parsed_data_file = os.path.join(data_folder, \"parsed_data.pkl\")\n",
    "image_data_file = os.path.join(data_folder, \"image_data.pkl\")\n",
    "\n",
    "# Ensure folders exist\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "if not os.path.exists(image_folder):\n",
    "    os.makedirs(image_folder)\n",
    "\n",
    "\n",
    "def load_or_parse_text_data():\n",
    "    \"\"\"Load text data from pickle file or parse new documents\"\"\"\n",
    "    modification_time_of_parsed_data = os.stat(parsed_data_file).st_mtime if os.path.exists(parsed_data_file) else 0\n",
    "\n",
    "    parse_data = False\n",
    "    docs_to_be_parsed_by_llamaparser = []\n",
    "\n",
    "    # Check if any files have been modified since last parsing\n",
    "    for file in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        \n",
    "        # Skip image folder, pickle files, and the images themselves\n",
    "        if file_path.endswith(\".pkl\") or file_path == image_folder or os.path.isdir(file_path):\n",
    "            continue\n",
    "\n",
    "        # Check if the file is an image\n",
    "        file_type = pathlib.Path(file).suffix.lower()\n",
    "        if file_type in ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']:\n",
    "            # Move image to image folder\n",
    "            image_dest_path = os.path.join(image_folder, os.path.basename(file_path))\n",
    "            if not os.path.exists(image_dest_path):\n",
    "                shutil.copy(file_path, image_dest_path)\n",
    "            continue\n",
    "\n",
    "        modification_time_of_file = os.stat(file_path).st_mtime\n",
    "        if modification_time_of_file > modification_time_of_parsed_data:\n",
    "            parse_data = True\n",
    "            break\n",
    "\n",
    "    # Get files to parse\n",
    "    for file in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        \n",
    "        # Skip image folder, pickle files, and the images themselves\n",
    "        if file_path.endswith(\".pkl\") or file_path == image_folder or os.path.isdir(file_path):\n",
    "            continue\n",
    "            \n",
    "        file_type = pathlib.Path(file).suffix.lower()\n",
    "        \n",
    "        # Skip images in main folder (they will be processed separately)\n",
    "        if file_type in ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']:\n",
    "            continue\n",
    "\n",
    "        # Include documents for text parsing\n",
    "        if file_type in [\n",
    "            '.pdf', '.doc', '.docx', '.docm', '.dot', '.dotx', '.dotm', '.rtf', '.wps', \n",
    "            '.wpd', '.sxw', '.stw', '.sxg', '.pages', '.mw', '.mcw', '.uot', '.uof', \n",
    "            '.uos', '.uop', '.ppt', '.pptx', '.pot', '.pptm', '.potx', '.potm', '.key', \n",
    "            '.odp', '.odg', '.otp', '.fopd', '.sxi', '.sti', '.epub', '.html', '.htm'\n",
    "        ]:\n",
    "            docs_to_be_parsed_by_llamaparser.append(file_path)\n",
    "        elif file_type in ['.csv', '.xls', '.xlsx']:\n",
    "            docs_to_be_parsed_by_llamaparser.append(file_path)\n",
    "        else:\n",
    "            print(\"Cannot parse this file type:\", file, file_type)\n",
    "            continue\n",
    "\n",
    "    if not parse_data and os.path.exists(parsed_data_file):\n",
    "        print(\"Loading parsed text data\")\n",
    "        with open(parsed_data_file, \"rb\") as f:\n",
    "            parsed_data = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Parsing text data\")\n",
    "        print(docs_to_be_parsed_by_llamaparser)\n",
    "        \n",
    "        llama_parse = LlamaParse(api_key=llamaparse_api_key, result_type=\"markdown\")\n",
    "        \n",
    "        try:\n",
    "            llama_parse_documents = llama_parse.load_data(docs_to_be_parsed_by_llamaparser)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during parsing: {e}\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Parsed {len(llama_parse_documents)} documents\")\n",
    "\n",
    "        with open(parsed_data_file, \"wb\") as f:\n",
    "            pickle.dump(llama_parse_documents, f)\n",
    "        \n",
    "        parsed_data = llama_parse_documents\n",
    "\n",
    "    # Make sure we're returning a list\n",
    "    if isinstance(parsed_data, dict):\n",
    "        # Convert dictionary to list of Document objects\n",
    "        return [Document(text=str(value), metadata={\"source\": key}) for key, value in parsed_data.items()]\n",
    "    elif not isinstance(parsed_data, list):\n",
    "        # If it's not a list or dict, wrap it in a list\n",
    "        return [parsed_data]\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "# Initialize CLIP model and processor\n",
    "class CLIPImageEmbedder:\n",
    "    def __init__(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_image_embedding(self, image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_features = self.model.get_image_features(**inputs)\n",
    "                \n",
    "            # Normalize embeddings\n",
    "            image_embeddings = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "            \n",
    "            # Convert to numpy array and resize to match expected dimensions\n",
    "            embedding = image_embeddings.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Resize to match the expected dimension (128)\n",
    "            if len(embedding) > 128:\n",
    "                embedding = embedding[:128]\n",
    "            elif len(embedding) < 128:\n",
    "                embedding = np.pad(embedding, (0, 128 - len(embedding)))\n",
    "                \n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_embedder = CLIPImageEmbedder(clip_model, clip_processor)\n",
    "\n",
    "def process_image_data():\n",
    "    \"\"\"Process image data without storing embeddings in metadata\"\"\"\n",
    "    images_data = []\n",
    "    \n",
    "    # Check if we already have processed images\n",
    "    if os.path.exists(image_data_file):\n",
    "        with open(image_data_file, \"rb\") as f:\n",
    "            existing_data = pickle.load(f)\n",
    "        \n",
    "        # Get existing image paths\n",
    "        existing_paths = set()\n",
    "        for item in existing_data:\n",
    "            if isinstance(item, Document):\n",
    "                existing_paths.add(item.metadata.get(\"image_path\", \"\"))\n",
    "            elif isinstance(item, dict):\n",
    "                existing_paths.add(item.get(\"image_path\", \"\"))\n",
    "    else:\n",
    "        existing_data = []\n",
    "        existing_paths = set()\n",
    "    \n",
    "    # Process any new images\n",
    "    for file in os.listdir(image_folder):\n",
    "        file_path = os.path.join(image_folder, file)\n",
    "        \n",
    "        # Skip if not an image or already processed\n",
    "        if not os.path.isfile(file_path) or file_path in existing_paths:\n",
    "            continue\n",
    "            \n",
    "        file_type = pathlib.Path(file).suffix.lower()\n",
    "        if file_type not in ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']:\n",
    "            continue\n",
    "        \n",
    "        # Extract metadata from filename\n",
    "        filename = os.path.basename(file_path)\n",
    "        name_parts = os.path.splitext(filename)[0].replace('_', ' ').split()\n",
    "        \n",
    "        # Generate metadata from filename\n",
    "        image_type = ' '.join([part for part in name_parts if part in ['xray', 'mri', 'ct', 'scan', 'ultrasound']])\n",
    "        body_part = ' '.join([part for part in name_parts if part in ['chest', 'brain', 'abdomen', 'spine', 'knee', 'heart']])\n",
    "        condition = ' '.join([part for part in name_parts if part in ['normal', 'pneumonia', 'fracture', 'tumor', 'cancer']])\n",
    "        \n",
    "        # Create a document for each image with minimal metadata\n",
    "        metadata = {\n",
    "            \"image_path\": file_path,\n",
    "            \"filename\": filename,\n",
    "            \"image_type\": image_type if image_type else \"medical image\",\n",
    "            \"body_part\": body_part if body_part else \"\",\n",
    "            \"condition\": condition if condition else \"\",\n",
    "            \"is_image\": True\n",
    "        }\n",
    "        \n",
    "        # Get image embedding but don't include it in metadata\n",
    "        embedding = clip_embedder.get_image_embedding(file_path)\n",
    "        if embedding is not None:\n",
    "            # Create document with metadata but without embedding in metadata\n",
    "            doc = Document(\n",
    "                text=f\"Medical image: {metadata['image_type']} of {metadata['body_part']} {metadata['condition']}. Filename: {filename}\",\n",
    "                metadata=metadata\n",
    "            )\n",
    "            images_data.append(doc)\n",
    "    \n",
    "    # Combine with existing data\n",
    "    all_image_data = []\n",
    "    for item in existing_data:\n",
    "        if isinstance(item, Document):\n",
    "            # Ensure no embeddings in metadata\n",
    "            if item.metadata and \"embedding\" in item.metadata:\n",
    "                del item.metadata[\"embedding\"]\n",
    "            all_image_data.append(item)\n",
    "    all_image_data.extend(images_data)\n",
    "    \n",
    "    # Save processed data\n",
    "    with open(image_data_file, \"wb\") as f:\n",
    "        pickle.dump(all_image_data, f)\n",
    "    \n",
    "    return all_image_data\n",
    "\n",
    "# Set up text embeddings\n",
    "embed_model = NomicEmbedding(\n",
    "    api_key=\"Your nomic api key\",\n",
    "    dimensionality=128,\n",
    "    model_name=\"nomic-embed-text-v1.5\",\n",
    ")\n",
    "\n",
    "# Configure settings\n",
    "Settings.embed_model = embed_model\n",
    "llm = Groq(model=\"llama-3.3-70b-versatile\", api_key=groq_api_key, stream=True)\n",
    "Settings.llm = llm\n",
    "\n",
    "# Process data\n",
    "text_documents = load_or_parse_text_data()\n",
    "image_documents = process_image_data()\n",
    "\n",
    "# Ensure no document has embeddings in metadata\n",
    "for doc in image_documents + text_documents:\n",
    "    if isinstance(doc, Document) and doc.metadata and \"embedding\" in doc.metadata:\n",
    "        del doc.metadata[\"embedding\"]\n",
    "\n",
    "# Combine documents\n",
    "all_documents = text_documents + image_documents\n",
    "print(f\"Loaded {len(text_documents)} text documents and {len(image_documents)} image documents\")\n",
    "\n",
    "# Initialize Qdrant Client\n",
    "client = qdrant_client.QdrantClient(api_key=qdrant_api_key, url=qdrant_url)\n",
    "print(f\"Connected to Qdrant: {client.get_collections()}\")\n",
    "\n",
    "# Create a unique collection name\n",
    "collection_name = f\"medical_rag_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Create index without using SentenceSplitter explicitly\n",
    "try:\n",
    "    # Create a new vector store\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    # Create index with default settings\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=all_documents, \n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    \n",
    "    query_engine = index.as_query_engine()\n",
    "    print(f\"Created index with collection name: {collection_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")\n",
    "    # Print more detailed error information\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \"\"\"Convert an image to base64 for display in HTML\"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "def move_file(file_path, destination_folder):\n",
    "    \"\"\"Move uploaded file to data folder\"\"\"\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        \n",
    "    destination_path = os.path.join(destination_folder, os.path.basename(file_path))\n",
    "    shutil.copy(file_path, destination_path)\n",
    "    print(f\"Copied {file_path} to {destination_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f55ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07598a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model loading...\n",
      "Loading model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import base64\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import traceback\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define a pastel color theme with only supported properties\n",
    "pastel_theme = gr.themes.Soft(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"blue\",\n",
    "    neutral_hue=\"slate\",\n",
    "    radius_size=gr.themes.sizes.radius_sm,\n",
    ").set(\n",
    "    body_background_fill=\"linear-gradient(to right, #e9f5f9, #f5f0fe)\",\n",
    "    block_background_fill=\"#ffffff\",\n",
    "    block_label_background_fill=\"#f0f7ff\",\n",
    "    block_title_text_color=\"#6b7280\",\n",
    "    button_primary_background_fill=\"#bcd8ff\",\n",
    "    button_primary_background_fill_hover=\"#a3c8ff\",\n",
    "    button_primary_text_color=\"#1f2937\",\n",
    "    button_secondary_background_fill=\"#f0f7ff\",\n",
    "    button_secondary_background_fill_hover=\"#e0ebfa\",\n",
    "    button_secondary_text_color=\"#4b5563\",\n",
    "    input_background_fill=\"#f7fafc\",\n",
    ")\n",
    "\n",
    "# ----- Functions for first tab (RAG system) -----\n",
    "def predict(message, history, request=None):\n",
    "    \"\"\"Process user query and return appropriate response\"\"\"\n",
    "    try:\n",
    "        # Handle input message\n",
    "        query_text = message.lower() if isinstance(message, str) else message['text'].lower()\n",
    "        \n",
    "        # Process query\n",
    "        response = query_engine.query(query_text)\n",
    "        response_text = str(response)\n",
    "        \n",
    "        # Image handling\n",
    "        image_keywords = [\"image\", \"xray\", \"x-ray\", \"scan\", \"mri\", \"ct scan\", \"ultrasound\", \"show me\", \"display\"]\n",
    "        is_image_request = any(keyword in query_text for keyword in image_keywords)\n",
    "        \n",
    "        if is_image_request and hasattr(response, 'source_nodes'):\n",
    "            image_nodes = [node for node in response.source_nodes if node.node.metadata.get(\"is_image\", False)]\n",
    "            \n",
    "            if image_nodes:\n",
    "                best_match = image_nodes[0]\n",
    "                image_path = best_match.node.metadata.get(\"image_path\")\n",
    "                \n",
    "                if image_path and os.path.exists(image_path):\n",
    "                    # Convert image to base64\n",
    "                    with open(image_path, \"rb\") as img_file:\n",
    "                        base64_img = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "                    \n",
    "                    # Return ONLY the image tag\n",
    "                    return {\"text\": f\"<img src='data:image/jpeg;base64,{base64_img}' style='max-width: 100%; max-height: 400px;'/>\"}\n",
    "        \n",
    "        # Text-only response\n",
    "        return {\"text\": response_text}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in predict: {e}\")\n",
    "        return {\"text\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "# Set up model paths and configurations\n",
    "MODEL_PATH = \"prithivMLmods/Radiology-Infer-Mini\"\n",
    "\n",
    "# Initialize model and processor on startup to avoid reloading\n",
    "def load_model():\n",
    "    print(\"Loading model and processor...\")\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        size={\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}\n",
    "    )\n",
    "    print(\"Model and processor loaded successfully!\")\n",
    "    return model, processor\n",
    "\n",
    "print(\"Starting model loading...\")\n",
    "model, processor = load_model()\n",
    "\n",
    "\n",
    "def analyze_image(image, custom_prompt=None):\n",
    "    \"\"\"Process the uploaded image and return the model's analysis\"\"\"\n",
    "    try:\n",
    "        if image is None:\n",
    "            return \"Please upload an image to analyze.\"\n",
    "        \n",
    "        print(f\"Image received: {type(image)}\")\n",
    "        prompt = custom_prompt if custom_prompt and custom_prompt.strip() else \"Describe this image.\"\n",
    "        print(f\"Using prompt: {prompt}\")\n",
    "        \n",
    "        # Convert to PIL image if needed\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "            print(\"Converted numpy array to PIL Image\")\n",
    "        \n",
    "        print(\"Creating messages structure...\")\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": image,\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Prepare for inference\n",
    "        print(\"Applying chat template...\")\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        print(\"Processing vision info...\")\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        print(\"Creating processor inputs...\")\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Move inputs to the same device as model\n",
    "        device = next(model.parameters()).device\n",
    "        print(f\"Moving inputs to device: {device}\")\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Generate output\n",
    "        print(\"Generating with model...\")\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "        \n",
    "        print(\"Processing generated output...\")\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Analysis completed. Output length: {len(output_text[0])}\")\n",
    "        return output_text[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_message = f\"Error during analysis: {str(e)}\\n\\n\"\n",
    "        error_message += traceback.format_exc()\n",
    "        print(error_message)\n",
    "        return error_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3e27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse this file type: parsed_data.pkl .pkl\n",
      "Loading parsed data\n",
      "1\n",
      "https://4bc6ac4e-f967-4ab5-afa1-75436961beeb.europe-west3-0.gcp.cloud.qdrant.io eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIiwiZXhwIjoxNzQ3ODAyNjI0fQ.xvfLP8fXj-eXhgh1dEz1CZ7lFqt5xVWtSEWS-gIUFjg\n",
      "<qdrant_client.qdrant_client.QdrantClient object at 0x000001D771CDAA50>\n",
      "collections=[CollectionDescription(name='d9fc8f03-9130-4bcf-9ba0-db3b7cfc85ac'), CollectionDescription(name='bd3c60b3-806a-4cb7-8d50-fea184921a6d'), CollectionDescription(name='b6a83a60-a7f2-4222-84e4-393529782a90'), CollectionDescription(name='41c46075-cb8c-4c83-8d84-6bfaca080254'), CollectionDescription(name='14d2d6d1-de78-4974-98a3-5f78e9d6b5a5'), CollectionDescription(name='a9979262-afc2-4d21-85f6-1b287bd3a4ba'), CollectionDescription(name='58f90b84-fdac-44da-89a2-660770eb1900'), CollectionDescription(name='f6c73f77-c8cd-4252-991a-e376a9646326'), CollectionDescription(name='8748951f-b908-4c44-bf37-88ff67d2adf4'), CollectionDescription(name='3c73a468-cfb0-4c83-9c12-a9a877f0dad7'), CollectionDescription(name='0458a2e3-635d-4f90-92b2-590785d35db0'), CollectionDescription(name='482dc406-53fd-454b-bda1-7b2d3059bf26'), CollectionDescription(name='86f3ebf4-c134-41c5-9649-887ec441b36c'), CollectionDescription(name='a5a694c0-0a19-49c8-bf5f-ce48555c46fc'), CollectionDescription(name='c8cd6bcc-77d0-45fc-9566-4e04aa9a908a'), CollectionDescription(name='c9db2cc2-eaad-4fb6-bf5e-4eae2f5e6e95'), CollectionDescription(name='53859b4a-6716-47c8-ad8c-322ad5f85e2c'), CollectionDescription(name='67174eed-7471-4caa-bf0f-0502d6e68d99'), CollectionDescription(name='fa832a0e-6d0a-45cf-96db-6ade890af40b'), CollectionDescription(name='528252f8-030f-491f-a755-e89b0a4823c4'), CollectionDescription(name='7d815af1-fa2a-484b-b84e-24fa6b358c89'), CollectionDescription(name='8a2b5d45-1523-4b92-a589-6e61c23022a9'), CollectionDescription(name='79f0d550-ab4d-4936-8534-7185a547c96c'), CollectionDescription(name='d9f593ba-6c85-4ffd-9080-4f1eb32dd2a4'), CollectionDescription(name='883c60d0-bb5f-4e0c-974a-4005d5fd59cd'), CollectionDescription(name='b1a1ea87-d3bb-4da1-be52-39589326d072'), CollectionDescription(name='140808d2-94c0-4b6f-98ee-790ad61f19b5'), CollectionDescription(name='66a46f3f-dc49-4e85-a038-9d7b32665fef'), CollectionDescription(name='7b9471e6-cd60-4d2d-ac03-e3514414b2f4'), CollectionDescription(name='d2f98084-a45d-44a4-976b-9f050ad03586'), CollectionDescription(name='c7fdd385-00a9-4314-b13d-a818a1e26b7c'), CollectionDescription(name='48823fbf-8d82-44c8-a2f2-35ab608f3f6d'), CollectionDescription(name='354f0e7a-561b-4e4b-9af6-d4aa100fa680'), CollectionDescription(name='403751f9-5ae0-44b4-83a5-fba607d4abe8'), CollectionDescription(name='72686765-dee5-4964-9829-9fcb14bd12a8'), CollectionDescription(name='c0a4beb8-2762-4c71-9d79-661c60703b67'), CollectionDescription(name='cbe512eb-eb3e-4794-805a-ba4e15d04348'), CollectionDescription(name='63f985d3-98ec-4e26-86d1-84ec05e7338d'), CollectionDescription(name='d7dc6cd3-cdeb-4699-b1c6-0f944ff3f852'), CollectionDescription(name='404e5bb8-8f47-498a-a2b4-0705efab87bc'), CollectionDescription(name='3e9b8714-09d9-4c10-8528-fced8e41a6ca'), CollectionDescription(name='769cab4d-e9b6-4105-abf2-c00fce5183d3'), CollectionDescription(name='2810983b-b35f-4bb8-8d40-919237be6bf9'), CollectionDescription(name='36a32bd0-0a33-4289-8696-18a3d3f1a117'), CollectionDescription(name='66002f6b-53c4-494a-8f3a-66e10e3b1304'), CollectionDescription(name='03fb78cf-6d5a-4034-9cc5-cb08e63899e6'), CollectionDescription(name='cb9b564a-f087-4cba-82bc-c1874f297557'), CollectionDescription(name='3ec7bc2d-b7f6-4065-a7cd-05b4017f7ad5'), CollectionDescription(name='a9a68163-fc07-4f9d-9140-43b6bacf3e6d'), CollectionDescription(name='30c4c499-cff8-4f7f-ac2a-03e89b60d754'), CollectionDescription(name='6d8e175e-b647-445b-ab12-602505684e86'), CollectionDescription(name='a8927c8e-b2a8-438f-b758-dedd291ac515'), CollectionDescription(name='0eb61a3d-a882-4db6-9f7c-ec31ea7eb5cf'), CollectionDescription(name='f24f010e-4dd2-4ab8-aa9a-5e0701d31740'), CollectionDescription(name='4f3d9846-9163-426a-9547-64081c7cc147'), CollectionDescription(name='74c5af63-5dcb-4b9e-aed4-e65f85e57ebf')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio  \n",
    "nest_asyncio.apply()\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "import qdrant_client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from llama_parse import LlamaParse\n",
    "llamaparse_api_key_med = \"Your llamaparse api key\"\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# Set the full path to your \"data\" folder\n",
    "data_folder_med = \"your med data folder\"\n",
    "data_file_med = os.path.join(data_folder_med, \"parsed_data.pkl\")\n",
    "\n",
    "def load_or_parse_data_med():\n",
    "    modification_time_of_parsed_data_med = os.stat(data_file_med).st_mtime if os.path.exists(data_file_med) else 0\n",
    "\n",
    "    parse_data_med = False\n",
    "    docs_to_be_parsed_by_llamaparser_med = []\n",
    "\n",
    "    for file_med in os.listdir(data_folder_med):\n",
    "        file_path_med = os.path.join(data_folder_med, file_med)\n",
    "\n",
    "        # Skip .pkl files\n",
    "        if file_path_med.endswith(\".pkl\"):\n",
    "            continue\n",
    "\n",
    "        modification_time_of_file_med = os.stat(file_path_med).st_mtime\n",
    "        if modification_time_of_file_med > modification_time_of_parsed_data_med:\n",
    "            parse_data_med = True\n",
    "            break\n",
    "\n",
    "    for file_med in os.listdir(data_folder_med):\n",
    "        file_path_med = os.path.join(data_folder_med, file_med)\n",
    "        file_type_med = pathlib.Path(file_med).suffix.lower()\n",
    "\n",
    "        if file_type_med in [\n",
    "            '.pdf', '.doc', '.docx', '.docm', '.dot', '.dotx', '.dotm', '.rtf', '.wps', \n",
    "            '.wpd', '.sxw', '.stw', '.sxg', '.pages', '.mw', '.mcw', '.uot', '.uof', \n",
    "            '.uos', '.uop', '.ppt', '.pptx', '.pot', '.pptm', '.potx', '.potm', '.key', \n",
    "            '.odp', '.odg', '.otp', '.fopd', '.sxi', '.sti', '.epub', '.html', '.htm'\n",
    "        ]:\n",
    "            docs_to_be_parsed_by_llamaparser_med.append(file_path_med)\n",
    "\n",
    "        elif file_type_med in ['.csv', '.xls', '.xlsx']:\n",
    "            # Directly add CSV files instead of converting them to HTML\n",
    "            docs_to_be_parsed_by_llamaparser_med.append(file_path_med)\n",
    "        \n",
    "        else:\n",
    "            print(\"Cannot parse this file type:\", file_med, file_type_med)\n",
    "            continue\n",
    "\n",
    "    if not parse_data_med:\n",
    "        print(\"Loading parsed data\")\n",
    "        with open(data_file_med, \"rb\") as f_med:\n",
    "            parsed_data_med = pickle.load(f_med)\n",
    "    else:\n",
    "        print(\"Parsing data\")\n",
    "        print(docs_to_be_parsed_by_llamaparser_med)\n",
    "        \n",
    "        llama_parse_med = LlamaParse(api_key=\"Your llamaparse api key\", result_type=\"markdown\")\n",
    "        \n",
    "        try:\n",
    "            llama_parse_documents_med = llama_parse_med.load_data(docs_to_be_parsed_by_llamaparser_med)\n",
    "        except Exception as e_med:\n",
    "            print(f\"Error during parsing: {e_med}\")\n",
    "            return []\n",
    "\n",
    "        print(type(llama_parse_documents_med))\n",
    "        print(llama_parse_documents_med)\n",
    "\n",
    "        with open(data_file_med, \"wb\") as f_med:\n",
    "            pickle.dump(llama_parse_documents_med, f_med)\n",
    "        \n",
    "        parsed_data_med = llama_parse_documents_med\n",
    "        print(f\"\\n\\nLoaded {len(parsed_data_med)} documents\")\n",
    "\n",
    "    return parsed_data_med\n",
    "\n",
    "llama_parse_documents_med = load_or_parse_data_med()\n",
    "\n",
    "print(len(llama_parse_documents_med))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "qdrant_url_med = \"Your qdrant url med\"\n",
    "qdrant_api_key_med = \"your qdrant api key med\"\n",
    "print(qdrant_url_med, qdrant_api_key_med)\n",
    "\n",
    "from llama_index.embeddings.nomic import NomicEmbedding\n",
    "embed_model_med = NomicEmbedding(\n",
    "    api_key=\"Your nomic api key\",\n",
    "    dimensionality=128,\n",
    "    model_name=\"nomic-embed-text-v1.5\",\n",
    ")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = embed_model_med\n",
    "from llama_index.llms.groq import Groq\n",
    "groq_api_key_med = \"your groq api key\"\n",
    "\n",
    "llm_med = Groq(model=\"llama-3.3-70b-versatile\", api_key=groq_api_key_med, stream=True)\n",
    "\n",
    "Settings.llm = llm_med\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Define API key and URL\n",
    "qdrant_api_key_med = \"your qdrant api key med\"\n",
    "qdrant_url_med = \"your qdrant url med\"\n",
    "\n",
    "# Initialize Qdrant Client\n",
    "client_med = QdrantClient(api_key=qdrant_api_key_med, url=qdrant_url_med)\n",
    "print(client_med)\n",
    "print(client_med.get_collections())\n",
    "\n",
    "import uuid\n",
    "collection_name_med = uuid.uuid4()\n",
    "try:\n",
    "    vector_store_med = QdrantVectorStore(client=client_med, collection_name=str(collection_name_med))\n",
    "    storage_context_med = StorageContext.from_defaults(vector_store=vector_store_med)\n",
    "    index_med = VectorStoreIndex.from_documents(documents=llama_parse_documents_med, storage_context=storage_context_med)\n",
    "except Exception as e_med:\n",
    "    print(e_med)\n",
    "\n",
    "query_engine_med = index_med.as_query_engine()\n",
    "\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import shutil\n",
    "\n",
    "def move_file_med(file_path_med, destination_folder_med):\n",
    "    # Create the destination folder if it does not exist\n",
    "    if not os.path.exists(destination_folder_med):\n",
    "        os.makedirs(destination_folder_med)\n",
    "        \n",
    "    print(file_path_med)\n",
    "    print(destination_folder_med)\n",
    "    # Move the file\n",
    "    destination_path_med = os.path.join(destination_folder_med, os.path.basename(file_path_med))\n",
    "    print(destination_path_med)\n",
    "    shutil.move(file_path_med, destination_path_med)\n",
    "    print(f\"Moved {file_path_med} to {destination_path_med}\")\n",
    "\n",
    "def predict_medication(message_med, history_med):\n",
    "    print(message_med)\n",
    "\n",
    "    if(message_med[\"files\"]):\n",
    "        for file_med in message_med[\"files\"]:\n",
    "            print(file_med)\n",
    "            move_file_med(file_med['path'], \"your med data path\")\n",
    "\n",
    "    response_med = query_engine_med.query(message_med['text'])\n",
    "    return str(response_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77298cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_uploaded_file(file):\n",
    "    \"\"\"Process an uploaded medical report file\"\"\"\n",
    "    if file is None:\n",
    "        return \"Please upload a file first.\"\n",
    "    \n",
    "    try:\n",
    "        # Get the filename and create destination path\n",
    "        filename = os.path.basename(file.name)\n",
    "        destination_path = os.path.join(data_folder, filename)\n",
    "        \n",
    "        # Check if file already exists\n",
    "        if os.path.exists(destination_path):\n",
    "            # Generate a unique filename to avoid overwriting\n",
    "            base, ext = os.path.splitext(filename)\n",
    "            unique_filename = f\"{base}_{uuid.uuid4().hex[:6]}{ext}\"\n",
    "            destination_path = os.path.join(data_folder, unique_filename)\n",
    "        \n",
    "        # Copy the uploaded file to the data folder\n",
    "        shutil.copy(file.name, destination_path)\n",
    "        \n",
    "        # Determine if the file is an image\n",
    "        file_ext = os.path.splitext(filename)[1].lower()\n",
    "        is_image = file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']\n",
    "        \n",
    "        if is_image:\n",
    "            # Move image to image folder\n",
    "            image_dest_path = os.path.join(image_folder, os.path.basename(destination_path))\n",
    "            shutil.copy(destination_path, image_dest_path)\n",
    "            \n",
    "            # Process image data\n",
    "            global image_documents\n",
    "            image_documents = process_image_data()\n",
    "            update_message = f\"Image '{filename}' uploaded and processed successfully.\"\n",
    "        else:\n",
    "            # Process text documents\n",
    "            update_message = f\"Document '{filename}' uploaded successfully. Processing...\"\n",
    "            \n",
    "            # Force regeneration of parsed data\n",
    "            if os.path.exists(parsed_data_file):\n",
    "                # Modify the timestamp to trigger reprocessing\n",
    "                os.utime(parsed_data_file, (0, 0))\n",
    "            \n",
    "            # Reprocess all text data\n",
    "            global text_documents\n",
    "            text_documents = load_or_parse_text_data()\n",
    "            update_message += \" Document processed.\"\n",
    "        \n",
    "        # Update combined documents\n",
    "        global all_documents\n",
    "        all_documents = text_documents + image_documents\n",
    "        \n",
    "        # Re-index all documents\n",
    "        global collection_name, client, index, query_engine\n",
    "        \n",
    "        # Generate a new collection name first\n",
    "        new_collection_name = f\"medical_rag_{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        # Create the collection first (important!)\n",
    "        try:\n",
    "            client.create_collection(\n",
    "                collection_name=new_collection_name,\n",
    "                vectors_config={\"size\": 128, \"distance\": \"Cosine\"}\n",
    "            )\n",
    "            print(f\"Created new collection: {new_collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating collection: {e}\")\n",
    "            # If we can't create a new collection, try to use the existing one\n",
    "            new_collection_name = collection_name\n",
    "        \n",
    "        # Only after creating the collection, try to delete the old one\n",
    "        if collection_name != new_collection_name:\n",
    "            try:\n",
    "                collections = client.get_collections()\n",
    "                collection_exists = any(c.name == collection_name for c in collections.collections)\n",
    "                if collection_exists:\n",
    "                    client.delete_collection(collection_name=collection_name)\n",
    "                    print(f\"Deleted old collection: {collection_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting collection: {e}\")\n",
    "        \n",
    "        # Update the collection name\n",
    "        collection_name = new_collection_name\n",
    "        \n",
    "        # Create new index with updated documents\n",
    "        vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents=all_documents, \n",
    "            storage_context=storage_context,\n",
    "        )\n",
    "        \n",
    "        query_engine = index.as_query_engine()\n",
    "        \n",
    "        update_message += f\" Index updated with {len(all_documents)} documents. Ready for queries about the new content.\"\n",
    "        return update_message\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Error processing file: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43c542b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvsss\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:290: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Gradio interface...\n",
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://f1febfeebb6b9edfda.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f1febfeebb6b9edfda.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in predict: Unexpected Response: 404 (Not Found)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Not found: Collection `medical_rag_21979386` doesn\\'t exist!\"},\"time\":2.076e-6}'\n",
      "Parsing text data\n",
      "['B:\\\\miniproject\\\\real\\\\real-medrep.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████| 1/1 [00:45<00:00, 45.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 14 documents\n",
      "Created new collection: medical_rag_a8fddaa8\n",
      "Image received: <class 'PIL.Image.Image'>\n",
      "Using prompt: Describe this image.\n",
      "Creating messages structure...\n",
      "Applying chat template...\n",
      "Processing vision info...\n",
      "Creating processor inputs...\n",
      "Moving inputs to device: cpu\n",
      "Generating with model...\n",
      "{'text': 'What are medications for diabetics', 'files': []}\n",
      "Processing generated output...\n",
      "Analysis completed. Output length: 94\n"
     ]
    }
   ],
   "source": [
    "# Combined CSS for both tabs (keeping your original CSS)\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    background-color: #f0f8ff; /* Light blue background */\n",
    "}\n",
    "#title {\n",
    "    font-size: 36px !important;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\n",
    "    color: #4e54c8;\n",
    "    margin: 20px 0;\n",
    "    padding: 10px;\n",
    "    background: linear-gradient(90deg, #e0f7fa, #bbdefb, #e0f7fa);\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "}\n",
    ".subtitle {\n",
    "    text-align: center;\n",
    "    font-size: 18px;\n",
    "    color: #5c6bc0;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".upload-box {\n",
    "    background-color: #e3f2fd !important; /* Light blue for upload box */\n",
    "    border: 2px dashed #90caf9 !important;\n",
    "    border-radius: 10px !important;\n",
    "}\n",
    ".result-box {\n",
    "    background-color: #e8f5e9 !important; /* Light green for results */\n",
    "    border-radius: 10px !important;\n",
    "    font-size: 18px !important; \n",
    "    line-height: 1.5 !important; \n",
    "}\n",
    ".input-box {\n",
    "    background-color: #f3e5f5 !important; /* Light purple for input */\n",
    "    border-radius: 10px !important;\n",
    "}\n",
    ".button-primary {\n",
    "    background-color: #7986cb !important;\n",
    "    color: white !important;\n",
    "    border-radius: 8px !important;\n",
    "    padding: 10px 20px !important;\n",
    "    font-size: 16px !important;\n",
    "    font-weight: bold !important;\n",
    "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1) !important;\n",
    "}\n",
    ".button-primary:hover {\n",
    "    background-color: #5c6bc0 !important;\n",
    "    box-shadow: 0 6px 8px rgba(0, 0, 0, 0.15) !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create the Gradio interface with tabs\n",
    "with gr.Blocks(title=\"MULTIMODAL MEDICAL ASSISTANT\", theme=pastel_theme, css=css) as demo:\n",
    "    gr.HTML('<h1 id=\"title\">MULTIMODAL MEDICAL ASSISTANT</h1>')\n",
    "    \n",
    "    with gr.Tabs() as tabs:\n",
    "        # Tab 1: RAG System with File Upload\n",
    "        with gr.Tab(\"MEDICAL REPORT SUMMARIZATION\"):\n",
    "            gr.HTML(\n",
    "                \"\"\"<h1 class=\"header-text\" style=\"text-align: center; font-size: 2rem; color: #4b5563;\">\n",
    "                    MEDICAL REPORT SUMMARIZATION\n",
    "                </h1>\"\"\"\n",
    "            )\n",
    "            \n",
    "            # Add file upload section - FIXED STRUCTURE\n",
    "            with gr.Row():\n",
    "                # Left column for file upload\n",
    "                upload_col1 = gr.Column(scale=1)\n",
    "                with upload_col1:\n",
    "                    upload_file = gr.File(\n",
    "                        label=\"Upload Medical Report or Image\",\n",
    "                        file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".jpg\", \".png\", \".jpeg\"],\n",
    "                        elem_classes=[\"upload-box\"]\n",
    "                    )\n",
    "                    upload_button = gr.Button(\n",
    "                        \"Process Uploaded File\", \n",
    "                        variant=\"primary\",\n",
    "                        elem_classes=[\"button-primary\"]\n",
    "                    )\n",
    "                \n",
    "                # Right column for status\n",
    "                upload_col2 = gr.Column(scale=1)\n",
    "                with upload_col2:\n",
    "                    upload_status = gr.Textbox(\n",
    "                        label=\"Upload Status\",\n",
    "                        placeholder=\"Upload a medical report to begin...\",\n",
    "                        lines=3,\n",
    "                        elem_classes=[\"result-box\"]\n",
    "                    )\n",
    "            \n",
    "            # Connect the upload button to function\n",
    "            upload_button.click(\n",
    "                fn=process_uploaded_file,\n",
    "                inputs=[upload_file],\n",
    "                outputs=[upload_status]\n",
    "            )\n",
    "            \n",
    "            # Horizontal line separator\n",
    "            gr.HTML(\"<hr style='margin: 20px 0; border: 0; border-top: 1px solid #e0e0e0;'>\")\n",
    "            \n",
    "            # Create chat interface (original code)\n",
    "            chat = gr.ChatInterface(\n",
    "                fn=predict,\n",
    "                examples=[\n",
    "                    [\"What is condition of patient\"],\n",
    "                    [\"What is the dataset\"],\n",
    "                    [\"Show me chest xray images\"],\n",
    "                    [\"Summarize the medical report\"]\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        # Tab 2: Medical Image Analysis (Keep your original code)\n",
    "        with gr.Tab(\"MEDICAL IMAGE ANALYSIS\"):\n",
    "            gr.HTML(\n",
    "                \"\"\"<h1 class=\"header-text\" style=\"text-align: center; font-size: 2rem; color: #4b5563;\">\n",
    "                    MEDICAL IMAGE ANALYSIS\n",
    "                </h1>\"\"\"\n",
    "            )\n",
    "            gr.HTML(\"<div class='subtitle'>Upload a medical image to get analysis.</div>\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    image_input = gr.Image(\n",
    "                        type=\"pil\", \n",
    "                        label=\"UPLOAD MEDICAL IMAGE\",\n",
    "                        height=400,\n",
    "                        elem_classes=[\"upload-box\"]\n",
    "                    )\n",
    "                    prompt_input = gr.Textbox(\n",
    "                        label=\"Ask your query here\", \n",
    "                        placeholder=\"Describe this image.\",\n",
    "                        lines=2,\n",
    "                        elem_classes=[\"input-box\"]\n",
    "                    )\n",
    "                    analyze_button = gr.Button(\n",
    "                        \"Analyze Image\", \n",
    "                        variant=\"primary\",\n",
    "                        elem_classes=[\"button-primary\"]\n",
    "                    )\n",
    "                    \n",
    "                with gr.Column(scale=1):\n",
    "                    output = gr.Textbox(\n",
    "                        label=\"Analysis Results\", \n",
    "                        placeholder=\"Analysis will appear here...\",\n",
    "                        lines=15,\n",
    "                        elem_classes=[\"result-box\"]\n",
    "                    )\n",
    "            \n",
    "            # Set up the function call\n",
    "            analyze_button.click(\n",
    "                fn=analyze_image,\n",
    "                inputs=[image_input, prompt_input],\n",
    "                outputs=output\n",
    "            )\n",
    "            \n",
    "            # Alternative trigger - analyze on image upload too\n",
    "            image_input.change(\n",
    "                fn=analyze_image,\n",
    "                inputs=[image_input, prompt_input],\n",
    "                outputs=output\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"MEDICATIONS\", elem_classes=[\"medication-tab\"]):\n",
    "            gr.HTML(\n",
    "                \"\"\"<h1 class=\"header-text\" style=\"text-align: center; font-size: 2rem; color: #4b5563;\">\n",
    "                    MEDICATIONS\n",
    "                </h1>\"\"\"\n",
    "            )\n",
    "            gr.HTML(\"<div class='subtitle'>Ask about medications.</div>\")\n",
    "            \n",
    "            # Create medication chat interface\n",
    "            medication_chat = gr.ChatInterface(\n",
    "                fn=predict_medication,\n",
    "                examples=[\n",
    "                    \"What are medications for fever\",\n",
    "                    \"What are medications for diabetics\"\n",
    "                ],\n",
    "                title=\"\",  # Empty title since we already have the header\n",
    "                multimodal=True\n",
    "            )\n",
    "            \n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Launching Gradio interface...\")\n",
    "    \n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
